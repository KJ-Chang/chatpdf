import torch
import gradio as gr
from utils.chunks import chunk_manual
from utils.prompt import build_prompt
from src.pdf_reader import extract_text_from_pdf
from src.embedder import get_embedder_by_sentence_transformer
from src.generator import load_model_and_tokenizer, generate
from src.vector_store import create_faiss_index_native

class ChatPDFState:
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.chunks = None
        self.index = None
        self.embedder = None
        # åœ¨åˆå§‹åŒ–æ™‚å°±è¼‰å…¥æ¨¡å‹å’Œtokenizer
        print("æ­£åœ¨è¼‰å…¥èªè¨€æ¨¡å‹...")
        self.model, self.tokenizer = load_model_and_tokenizer('tiiuae/falcon-7b-instruct', self.device)
        print("èªè¨€æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

# å»ºç«‹å…¨åŸŸç‹€æ…‹
state = ChatPDFState()

def process_pdf(pdf_file):
    if pdf_file is None:
        return "è«‹ä¸Šå‚³PDFæ–‡ä»¶"
    
    # è®€å–ä¸Šå‚³çš„PDFæ–‡ä»¶
    text = extract_text_from_pdf(pdf_file.name)
    state.chunks = chunk_manual(text, 500, 50)
    
    # å‰µå»ºæ–‡ä»¶çš„ embeddings
    state.embedder = get_embedder_by_sentence_transformer('sentence-transformers/all-mpnet-base-v2', state.device)
    document_embeddings = state.embedder.encode(state.chunks, batch_size=2, convert_to_numpy=True)
    
    # å‰µå»ºå‘é‡ç´¢å¼•
    state.index = create_faiss_index_native(document_embeddings, state.device)
    
    return "PDFæ–‡ä»¶å·²è™•ç†å®Œæˆï¼Œç¾åœ¨å¯ä»¥é–‹å§‹æå•"

def answer_question(question):
    if state.index is None or state.chunks is None or state.embedder is None:
        return "è«‹å…ˆä¸Šå‚³ä¸¦è™•ç†PDFæ–‡ä»¶"
    
    # è™•ç†ç”¨æˆ¶å•é¡Œ
    query_embeddings = state.embedder.encode([question], batch_size=2, convert_to_numpy=True)
    
    # æ‰¾å‡ºç›¸é—œæ–‡æœ¬
    contexts = []
    D, I = state.index.search(query_embeddings, k=5)
    for d, i in zip(D[0], I[0]):
        contexts.append(state.chunks[i])

    # ä½¿ç”¨å·²è¼‰å…¥çš„æ¨¡å‹ç”Ÿæˆå›ç­”
    prompt = build_prompt(question, contexts)
    response = generate(state.model, state.tokenizer, prompt, state.device)
    
    return response

def create_interface():
    with gr.Blocks(title="MyChatPDF") as demo:
        gr.Markdown("# ğŸ“š MyChatPDF")
        gr.Markdown("ç¬¬ä¸€æ­¥ï¼šä¸Šå‚³ PDF æ–‡ä»¶é€²è¡Œå‘é‡åŒ–è™•ç†")
        
        with gr.Row():
            pdf_input = gr.File(label="ä¸Šå‚³ PDF æ–‡ä»¶", file_types=[".pdf"])
            process_btn = gr.Button("è™•ç†æ–‡ä»¶")
        
        with gr.Row():
            process_output = gr.Textbox(label="è™•ç†ç‹€æ…‹", lines=2)
            
        gr.Markdown("ç¬¬äºŒæ­¥ï¼šæå‡ºå•é¡Œ")
        
        with gr.Row():
            question_input = gr.Textbox(label="è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ", placeholder="è«‹å•...")
            submit_btn = gr.Button("æäº¤å•é¡Œ")
        
        with gr.Row():
            answer_output = gr.Textbox(label="å›ç­”", lines=5)
        
        process_btn.click(
            fn=process_pdf,
            inputs=[pdf_input],
            outputs=process_output
        )
        
        submit_btn.click(
            fn=answer_question,
            inputs=[question_input],
            outputs=answer_output
        )
    
    return demo

if __name__ == '__main__':
    demo = create_interface()
    demo.launch(share=False, server_name='0.0.0.0')  # è¨­ç½® share=False ä¾†ç¦ç”¨ public URL